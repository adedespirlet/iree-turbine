============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.0.0, pluggy-1.5.0
rootdir: /home/aurore/git/iree-turbine
configfile: setup.cfg
plugins: xdist-3.5.0
collected 1 item

tests/kernel/wave/attention/scatter_test.py start_indices [$T0*LOAD_ELEMS_PER_THREAD + $WG0*BLOCK_M + BLOCK_M*floor($T0/64)]
start_indices [$T0*LOAD_ELEMS_PER_THREAD + $WG0*BLOCK_M + BLOCK_M*floor($T0/64)]
index: {M: $T0*LOAD_ELEMS_PER_THREAD + $WG0*BLOCK_M + BLOCK_M*floor($T0/64) : LOAD_ELEMS_PER_THREAD : 1}
subs [($index0, $T0*LOAD_ELEMS_PER_THREAD + $WG0*BLOCK_M + BLOCK_M*floor($T0/64)), (M, 64), (N, 16), (BLOCK_M, 64), (BLOCK_N, 16), (LOAD_ELEMS_PER_THREAD, 1), (STORE_ELEMS_PER_THREAD, 1), (ADDRESS_SPACE, 1), ($T1, 0), ($T2, 0), ($WG0, 0), ($WG1, 0), ($WG2, 0)]
result_index {M: $T0 + 64*floor($T0/64)}
start_indices [$T0 + 64*floor($T0/64)]
#map = affine_map<()[s0] -> (s0 + (s0 floordiv 64) * 64)>
#translation = #iree_codegen.translation_info<pipeline = None workgroup_size = [64, 1, 1] subgroup_size = 64>
module attributes {transform.with_named_sequence} {
  stream.executable private @read_kernel {
    stream.executable.export public @read_kernel workgroups() -> (index, index, index) {
      %c1 = arith.constant 1 : index
      stream.return %c1, %c1, %c1 : index, index, index
    }
    builtin.module {
      func.func @read_kernel(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) attributes {translation_info = #translation} {
        %c0 = arith.constant 0 : index
        %thread_id_x = gpu.thread_id  x
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> memref<64xi32, strided<[1], offset: ?>>
        %1 = affine.apply #map()[%thread_id_x]
        %2 = vector.load %0[%1] : memref<64xi32, strided<[1], offset: ?>>, vector<1xi32>
        %3 = stream.binding.subspan %arg1[%c0] : !stream.binding -> memref<64xi32, strided<[1], offset: ?>>
        %4 = vector.load %3[%1] : memref<64xi32, strided<[1], offset: ?>>, vector<1xi32>
        %5 = stream.binding.subspan %arg2[%c0] : !stream.binding -> memref<64xi32, strided<[1], offset: ?>>
        %6 = vector.extract %4[0] : i32 from vector<1xi32>
        %7 = arith.index_cast %6 : i32 to index
        %8 = vector.extract %2[0] : i32 from vector<1xi32>
        %9 = memref.atomic_rmw addi %8, %5[%7] : (i32, memref<64xi32, strided<[1], offset: ?>>) -> i32
        return
      }
    }
  }
  func.func @isolated_benchmark(%arg0: tensor<64xi32>, %arg1: tensor<64xi32>, %arg2: tensor<64xi32>) {
    flow.dispatch @read_kernel::@read_kernel(%arg0, %arg1, %arg2) : (tensor<64xi32>, tensor<64xi32>, tensor<64xi32>) -> ()
    return
  }
}

Input a:
tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
        54, 55, 56, 57, 58, 59, 60, 61, 62, 63], dtype=torch.int32)
Input index:
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
Output:
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)
torch_output:
tensor([   0, 2016,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0], device='cuda:0', dtype=torch.int32)
.

============================== 1 passed in 2.50s ===============================
