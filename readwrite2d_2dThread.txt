============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.0.0, pluggy-1.5.0
rootdir: /home/aurore/git/iree-turbine
configfile: setup.cfg
plugins: xdist-3.5.0
collected 1 item

tests/kernel/wave/attention/scatter_test.py #map = affine_map<()[s0] -> (s0 + (s0 floordiv 64) * 8)>
#map1 = affine_map<()[s0] -> (s0 * 9)>

ty1= 9   -8 = 1
ty2= 18  -2*8= 2
ty3= 27  -3*8= 3 
...
#map2 = affine_map<()[s0] -> (s0 + (s0 floordiv 64) * 8 + 1)>
#map3 = affine_map<()[s0] -> (s0 + (s0 floordiv 64) * 8 + 2)>
#map4 = affine_map<()[s0] -> (s0 + (s0 floordiv 64) * 8 + 3)>
#map5 = affine_map<()[s0] -> (s0 + (s0 floordiv 64) * 8 + 4)>
#map6 = affine_map<()[s0] -> (s0 + (s0 floordiv 64) * 8 + 5)>
#map7 = affine_map<()[s0] -> (s0 + (s0 floordiv 64) * 8 + 6)>
#map8 = affine_map<()[s0] -> (s0 + (s0 floordiv 64) * 8 + 7)>
#translation = #iree_codegen.translation_info<pipeline = None workgroup_size = [64, 8, 1] subgroup_size = 64>
module attributes {transform.with_named_sequence} {
  stream.executable private @read_kernel {
    stream.executable.export public @read_kernel workgroups() -> (index, index, index) {
      %c1 = arith.constant 1 : index
      stream.return %c1, %c1, %c1 : index, index, index
    }
    builtin.module {
      func.func @read_kernel(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) attributes {translation_info = #translation} {
        %c0 = arith.constant 0 : index
        %thread_id_x = gpu.thread_id  x
        %thread_id_y = gpu.thread_id  y
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> memref<8x8xi32, strided<[8, 1], offset: ?>>
        %1 = affine.apply #map()[%thread_id_x]

        %2 = affine.apply #map1()[%thread_id_y]
        %3 = vector.load %0[%1, %2] : memref<8x8xi32, strided<[8, 1], offset: ?>>, vector<1xi32>

        vector.load [%1, 9] ====> vector.load [%1, 1]
        vector.load [%1, 18] ====> vector.load [%1, 2]
        vector.load [%1, 27] ====> vector.load [%1, 3]


        %4 = affine.apply #map2()[%thread_id_x]
        %5 = vector.load %0[%4, %2] : memref<8x8xi32, strided<[8, 1], offset: ?>>, vector<1xi32>
        %6 = affine.apply #map3()[%thread_id_x]
        %7 = vector.load %0[%6, %2] : memref<8x8xi32, strided<[8, 1], offset: ?>>, vector<1xi32>
        %8 = affine.apply #map4()[%thread_id_x]
        %9 = vector.load %0[%8, %2] : memref<8x8xi32, strided<[8, 1], offset: ?>>, vector<1xi32>
        %10 = affine.apply #map5()[%thread_id_x]
        %11 = vector.load %0[%10, %2] : memref<8x8xi32, strided<[8, 1], offset: ?>>, vector<1xi32>
        %12 = affine.apply #map6()[%thread_id_x]
        %13 = vector.load %0[%12, %2] : memref<8x8xi32, strided<[8, 1], offset: ?>>, vector<1xi32>
        %14 = affine.apply #map7()[%thread_id_x]
        %15 = vector.load %0[%14, %2] : memref<8x8xi32, strided<[8, 1], offset: ?>>, vector<1xi32>
        %16 = affine.apply #map8()[%thread_id_x]
        %17 = vector.load %0[%16, %2] : memref<8x8xi32, strided<[8, 1], offset: ?>>, vector<1xi32>
        %18 = stream.binding.subspan %arg3[%c0] : !stream.binding -> memref<8x8xi32, strided<[8, 1], offset: ?>>
        vector.store %3, %18[%1, %2] : memref<8x8xi32, strided<[8, 1], offset: ?>>, vector<1xi32>
        vector.store %5, %18[%4, %2] : memref<8x8xi32, strided<[8, 1], offset: ?>>, vector<1xi32>
        vector.store %7, %18[%6, %2] : memref<8x8xi32, strided<[8, 1], offset: ?>>, vector<1xi32>
        vector.store %9, %18[%8, %2] : memref<8x8xi32, strided<[8, 1], offset: ?>>, vector<1xi32>
        vector.store %11, %18[%10, %2] : memref<8x8xi32, strided<[8, 1], offset: ?>>, vector<1xi32>
        vector.store %13, %18[%12, %2] : memref<8x8xi32, strided<[8, 1], offset: ?>>, vector<1xi32>
        vector.store %15, %18[%14, %2] : memref<8x8xi32, strided<[8, 1], offset: ?>>, vector<1xi32>
        vector.store %17, %18[%16, %2] : memref<8x8xi32, strided<[8, 1], offset: ?>>, vector<1xi32>
        return
      }
    }
  }
  func.func @isolated_benchmark(%arg0: tensor<8x8xi32>, %arg1: tensor<8xi32>, %arg2: tensor<8x8xi32>, %arg3: tensor<8x8xi32>) -> tensor<8x8xi32> {
    %0 = flow.dispatch @read_kernel::@read_kernel(%arg0, %arg1, %arg2, %arg3) : (tensor<8x8xi32>, tensor<8xi32>, tensor<8x8xi32>, tensor<8x8xi32>) -> %arg3
    return %0 : tensor<8x8xi32>
  }
}

Input a:
tensor([[ 0,  1,  2,  3,  4,  5,  6,  7],
        [ 8,  9, 10, 11, 12, 13, 14, 15],
        [16, 17, 18, 19, 20, 21, 22, 23],
        [24, 25, 26, 27, 28, 29, 30, 31],
        [32, 33, 34, 35, 36, 37, 38, 39],
        [40, 41, 42, 43, 44, 45, 46, 47],
        [48, 49, 50, 51, 52, 53, 54, 55],
        [56, 57, 58, 59, 60, 61, 62, 63]], dtype=torch.int32)
Output:
tensor([[ 0,  0,  0,  0,  0,  0,  0,  0],
        [ 8,  9,  0,  0,  0,  0,  0,  0],
        [16, 17, 18,  0,  0,  0,  0,  0],
        [24, 25, 26, 27,  0,  0,  0,  0],
        [32, 33, 34, 35, 36,  0,  0,  0],
        [40, 41, 42, 43, 44, 45,  0,  0],
        [48, 49, 50, 51, 52, 53, 54,  0],
        [56, 57, 58, 59, 60, 61, 62, 63]], dtype=torch.int32)
F

=================================== FAILURES ===================================
____________________________ test_read_actual_data _____________________________

    @require_e2e
    def test_read_actual_data():
        # Constraints
        constraints = [
            tkw.HardwareConstraint(threads_per_wave=64, waves_per_block=(1, 8, 1), vector_shapes={M: 1, N: 1}),
            tkw.WorkgroupConstraint(M, BLOCK_M, 0),
            tkw.WorkgroupConstraint(N, BLOCK_N, 1),
            tkw.WaveConstraint(M, BLOCK_M),
            tkw.WaveConstraint(N, BLOCK_N),
        ]
    
        # Mapping (identity)
        i = tkw.IndexMapping.iterator(0)
        j = tkw.IndexMapping.iterator(1)
        mapping = tkw.IndexMapping(
            num_iterators=2,
            inputs={M: i,N:j},
            outputs={M: i,N:j},
        )
    
        # Define kernel
        @tkw.wave(constraints)
        def read_kernel(
            a: tkl.Memory[M, N, GLOBAL_ADDRESS_SPACE, tkl.i32],
            index: tkl.Memory[M,GLOBAL_ADDRESS_SPACE, tkl.i32],
            lds: tkl.Memory[M,N, ADDRESS_SPACE, tkl.i32],
            b: tkl.Memory[M, N, GLOBAL_ADDRESS_SPACE, tkl.i32],
        ):
    
            a_reg = tkw.read(a, elements_per_thread=LOAD_ELEMS_PER_THREAD,mapping=mapping)
    
            # index_reg=tkw.read(index,elements_per_thread=LOAD_ELEMS_PER_THREAD)
            # index_reg = tkw.broadcast(index_reg, target_shape=[M, N])
            # tkw.scatter_add(a_reg,index_reg,dim=0,memory=lds,mapping=mapping,elements_per_thread=LOAD_ELEMS_PER_THREAD)
    
            # lds_reg=tkw.read(lds, elements_per_thread=LOAD_ELEMS_PER_THREAD,mapping=mapping)
            # tkw.write(lds_reg,b,elements_per_thread=STORE_ELEMS_PER_THREAD,mapping=mapping)
    
            tkw.write(a_reg,b,elements_per_thread=STORE_ELEMS_PER_THREAD,mapping=mapping)
    
            return
        # Compile kernel
        options = WaveCompileOptions(
            subs={
                M: 8,
                N: 8,
                BLOCK_M: 8,
                BLOCK_N: 8,
                LOAD_ELEMS_PER_THREAD: 1,
                STORE_ELEMS_PER_THREAD: 1,
                ADDRESS_SPACE: tkl.AddressSpace.SHARED_MEMORY.value,
    
            },
            kernel_usages=[
            tkl.kernel_buffer.KernelBufferUsage.INPUT,
            tkl.kernel_buffer.KernelBufferUsage.INPUT,
            tkl.kernel_buffer.KernelBufferUsage.INPUT,
            tkl.kernel_buffer.KernelBufferUsage.OUTPUT,
            ],
            compile_to_mlir=False,
            canonicalize=True,
            run_bench=False,
        )
        options = set_default_run_config(options)
    
        read_fn = wave_compile(options, read_kernel)
        print(read_fn.asm)
        # Input tensors
    
        #index = device_randint(64, dtype=torch.int32).reshape(8, 8)
        #index=device_randint(0, 10, (16, 16), dtype=torch.int32)
        input = device_arange(8*8, dtype=torch.int32).reshape(8, 8).contiguous()
    
        index = device_arange(8, dtype=torch.int32).contiguous()
        lds= device_zeros((8, 8), dtype=torch.int32).reshape(8,8).contiguous()
        output = device_zeros((8, 8), dtype=torch.int32).reshape(8, 8).contiguous()
    
        # index=device_randint(0, 64, (32, 32), dtype=torch.int32)
        # output = device_zeros((32, 32), dtype=torch.int32).reshape(32, 32).contiguous()
        # input = device_ones(32*32, dtype=torch.int32).reshape(32, 32).contiguous()
        # Run kernel
        read_fn(input,index,lds,output)
    
        print("Input a:")
        print(input.cpu())
        # print("Input index:")
        # print(index.cpu())
        print("Output:")
        print(output.cpu()) #moves pytorch tensor from gpu back to cpu
    
        # Expected output (should match a)
>       torch.testing.assert_close(output, input)
E       AssertionError: Tensor-likes are not equal!
E       
E       Mismatched elements: 28 / 64 (43.8%)
E       Greatest absolute difference: 55 at index (6, 7)
E       Greatest relative difference: 1.0 at index (0, 1)

tests/kernel/wave/attention/scatter_test.py:152: AssertionError
=========================== short test summary info ============================
FAILED tests/kernel/wave/attention/scatter_test.py::test_read_actual_data - A...
============================== 1 failed in 2.47s ===============================
